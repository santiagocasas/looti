{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create looti emulators for observables dependent on z & k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: linear power spectrum $P_{lin}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Create a pandas dataframe from data files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load looti module read_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from looti import read_files as rf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to the data files and other specifications are passed to looti via a yaml file.\n",
    "The yaml file should contain the following information:\n",
    "- main_dir: the directory in which all relevant folders and files are stored\n",
    "- config_file: this file should include a section called where the varying parameters are specified\n",
    "\n",
    "- folders_path: path to the directory containing all the data folders, one for each class/camb run\n",
    "- params_file: each data folder should contain a file specifying the values of each parameter for this run\n",
    "- reference folder: name of the folder containing a run with fiducial values used as reference\n",
    "\n",
    "- z_file_name: file containing the redshift grid\n",
    "- k_file_name: file containing the k grid\n",
    "- data_file_name: file containing the data to be emulated\n",
    "- data_type: name of observable to be emulated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of \"FrameConstructor\" - the looti class used for data management.\n",
    "FrameConstructor takes the path to this yamle file as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameConstructor = rf.FrameConstructor(path_config_file='../readfile_configs/input4cast_lhs_Plin.yaml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe containing all training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataframe_ext \u001b[39m=\u001b[39m FrameConstructor\u001b[39m.\u001b[39mcreate_k_dataframe()\n\u001b[1;32m      2\u001b[0m dataframe_ext\n",
      "File \u001b[0;32m~/looti/looti/read_files.py:85\u001b[0m, in \u001b[0;36mFrameConstructor.create_k_dataframe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m     columns \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(k_array)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     84\u001b[0m     df_temp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39mobservable, index\u001b[39m=\u001b[39mmultiIndex1, columns\u001b[39m=\u001b[39mcolumns)\n\u001b[0;32m---> 85\u001b[0m     dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([dataframe, df_temp])\n\u001b[1;32m     88\u001b[0m dataframe\u001b[39m.\u001b[39mloc[\u001b[39m\"\u001b[39m\u001b[39mk_grid\u001b[39m\u001b[39m\"\u001b[39m,:] \u001b[39m=\u001b[39m k_array\n\u001b[1;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m dataframe\n",
      "File \u001b[0;32m~/miniconda3/envs/lootiemu/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/lootiemu/lib/python3.11/site-packages/pandas/core/reshape/concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m     objs,\n\u001b[1;32m    370\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[1;32m    379\u001b[0m )\n\u001b[0;32m--> 381\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/lootiemu/lib/python3.11/site-packages/pandas/core/reshape/concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[1;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_axes, concat_axis\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis, copy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m    618\u001b[0m )\n\u001b[1;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy:\n\u001b[1;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/miniconda3/envs/lootiemu/lib/python3.11/site-packages/pandas/core/internals/concat.py:223\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    217\u001b[0m vals \u001b[39m=\u001b[39m [ju\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mvalues \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units]\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m blk\u001b[39m.\u001b[39mis_extension:\n\u001b[1;32m    220\u001b[0m     \u001b[39m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[39m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[39m#  than concat_compat\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(vals, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    224\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     values \u001b[39m=\u001b[39m concat_compat(vals, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataframe_ext = FrameConstructor.create_k_dataframe()\n",
    "dataframe_ext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe_ext.to_csv(\"../data/rwth_output/class_Asw0wa_DP/Plin.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe containing reference at fiducial values for the varying parameters at each redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>791</th>\n",
       "      <th>792</th>\n",
       "      <th>793</th>\n",
       "      <th>794</th>\n",
       "      <th>795</th>\n",
       "      <th>796</th>\n",
       "      <th>797</th>\n",
       "      <th>798</th>\n",
       "      <th>799</th>\n",
       "      <th>800</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_type</th>\n",
       "      <th>redshift</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Plin</th>\n",
       "      <th>0.0</th>\n",
       "      <td>431.113317</td>\n",
       "      <td>437.961008</td>\n",
       "      <td>444.917352</td>\n",
       "      <td>451.984068</td>\n",
       "      <td>459.162899</td>\n",
       "      <td>466.455620</td>\n",
       "      <td>473.864033</td>\n",
       "      <td>481.389969</td>\n",
       "      <td>489.035283</td>\n",
       "      <td>496.801860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004483</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>0.003920</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.002997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>408.700290</td>\n",
       "      <td>415.191993</td>\n",
       "      <td>421.786702</td>\n",
       "      <td>428.486045</td>\n",
       "      <td>435.291678</td>\n",
       "      <td>442.205280</td>\n",
       "      <td>449.228561</td>\n",
       "      <td>456.363255</td>\n",
       "      <td>463.611122</td>\n",
       "      <td>470.973948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.002841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>387.361268</td>\n",
       "      <td>393.514041</td>\n",
       "      <td>399.764443</td>\n",
       "      <td>406.114018</td>\n",
       "      <td>412.564334</td>\n",
       "      <td>419.116984</td>\n",
       "      <td>425.773587</td>\n",
       "      <td>432.535786</td>\n",
       "      <td>439.405248</td>\n",
       "      <td>446.383670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.002693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15000000000000002</th>\n",
       "      <td>367.105848</td>\n",
       "      <td>372.936901</td>\n",
       "      <td>378.860479</td>\n",
       "      <td>384.878046</td>\n",
       "      <td>390.991087</td>\n",
       "      <td>397.201113</td>\n",
       "      <td>403.509657</td>\n",
       "      <td>409.918274</td>\n",
       "      <td>416.428548</td>\n",
       "      <td>423.042085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.003194</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>0.002553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>347.927195</td>\n",
       "      <td>353.453631</td>\n",
       "      <td>359.067759</td>\n",
       "      <td>364.770965</td>\n",
       "      <td>370.564660</td>\n",
       "      <td>376.450273</td>\n",
       "      <td>382.429258</td>\n",
       "      <td>388.503091</td>\n",
       "      <td>394.673270</td>\n",
       "      <td>400.941319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003620</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.002420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.8500000000000005</th>\n",
       "      <td>19.980626</td>\n",
       "      <td>20.298037</td>\n",
       "      <td>20.620487</td>\n",
       "      <td>20.948056</td>\n",
       "      <td>21.280824</td>\n",
       "      <td>21.618874</td>\n",
       "      <td>21.962290</td>\n",
       "      <td>22.311156</td>\n",
       "      <td>22.665558</td>\n",
       "      <td>23.025585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.9</th>\n",
       "      <td>19.644348</td>\n",
       "      <td>19.956417</td>\n",
       "      <td>20.273441</td>\n",
       "      <td>20.595497</td>\n",
       "      <td>20.922665</td>\n",
       "      <td>21.255026</td>\n",
       "      <td>21.592662</td>\n",
       "      <td>21.935657</td>\n",
       "      <td>22.284095</td>\n",
       "      <td>22.638063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.95</th>\n",
       "      <td>19.316442</td>\n",
       "      <td>19.623303</td>\n",
       "      <td>19.935035</td>\n",
       "      <td>20.251716</td>\n",
       "      <td>20.573423</td>\n",
       "      <td>20.900236</td>\n",
       "      <td>21.232237</td>\n",
       "      <td>21.569506</td>\n",
       "      <td>21.912129</td>\n",
       "      <td>22.260189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>18.996635</td>\n",
       "      <td>19.298416</td>\n",
       "      <td>19.604987</td>\n",
       "      <td>19.916425</td>\n",
       "      <td>20.232806</td>\n",
       "      <td>20.554209</td>\n",
       "      <td>20.880713</td>\n",
       "      <td>21.212399</td>\n",
       "      <td>21.549349</td>\n",
       "      <td>21.891647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k_grid</th>\n",
       "      <th></th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>...</td>\n",
       "      <td>43.129691</td>\n",
       "      <td>43.843880</td>\n",
       "      <td>44.569895</td>\n",
       "      <td>45.307931</td>\n",
       "      <td>46.058190</td>\n",
       "      <td>46.820871</td>\n",
       "      <td>47.596182</td>\n",
       "      <td>48.384332</td>\n",
       "      <td>49.185532</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      1           2           3           4    \\\n",
       "data_type redshift                                                              \n",
       "Plin      0.0                  431.113317  437.961008  444.917352  451.984068   \n",
       "          0.05                 408.700290  415.191993  421.786702  428.486045   \n",
       "          0.1                  387.361268  393.514041  399.764443  406.114018   \n",
       "          0.15000000000000002  367.105848  372.936901  378.860479  384.878046   \n",
       "          0.2                  347.927195  353.453631  359.067759  364.770965   \n",
       "...                                   ...         ...         ...         ...   \n",
       "          4.8500000000000005    19.980626   20.298037   20.620487   20.948056   \n",
       "          4.9                   19.644348   19.956417   20.273441   20.595497   \n",
       "          4.95                  19.316442   19.623303   19.935035   20.251716   \n",
       "          5.0                   18.996635   19.298416   19.604987   19.916425   \n",
       "k_grid                           0.000100    0.000102    0.000103    0.000105   \n",
       "\n",
       "                                      5           6           7           8    \\\n",
       "data_type redshift                                                              \n",
       "Plin      0.0                  459.162899  466.455620  473.864033  481.389969   \n",
       "          0.05                 435.291678  442.205280  449.228561  456.363255   \n",
       "          0.1                  412.564334  419.116984  425.773587  432.535786   \n",
       "          0.15000000000000002  390.991087  397.201113  403.509657  409.918274   \n",
       "          0.2                  370.564660  376.450273  382.429258  388.503091   \n",
       "...                                   ...         ...         ...         ...   \n",
       "          4.8500000000000005    21.280824   21.618874   21.962290   22.311156   \n",
       "          4.9                   20.922665   21.255026   21.592662   21.935657   \n",
       "          4.95                  20.573423   20.900236   21.232237   21.569506   \n",
       "          5.0                   20.232806   20.554209   20.880713   21.212399   \n",
       "k_grid                           0.000107    0.000109    0.000110    0.000112   \n",
       "\n",
       "                                      9           10   ...        791  \\\n",
       "data_type redshift                                     ...              \n",
       "Plin      0.0                  489.035283  496.801860  ...   0.004483   \n",
       "          0.05                 463.611122  470.973948  ...   0.004250   \n",
       "          0.1                  439.405248  446.383670  ...   0.004029   \n",
       "          0.15000000000000002  416.428548  423.042085  ...   0.003819   \n",
       "          0.2                  394.673270  400.941319  ...   0.003620   \n",
       "...                                   ...         ...  ...        ...   \n",
       "          4.8500000000000005    22.665558   23.025585  ...   0.000211   \n",
       "          4.9                   22.284095   22.638063  ...   0.000207   \n",
       "          4.95                  21.912129   22.260189  ...   0.000204   \n",
       "          5.0                   21.549349   21.891647  ...   0.000200   \n",
       "k_grid                           0.000114    0.000116  ...  43.129691   \n",
       "\n",
       "                                     792        793        794        795  \\\n",
       "data_type redshift                                                          \n",
       "Plin      0.0                   0.004287   0.004099   0.003920   0.003749   \n",
       "          0.05                  0.004065   0.003887   0.003717   0.003554   \n",
       "          0.1                   0.003853   0.003685   0.003524   0.003369   \n",
       "          0.15000000000000002   0.003652   0.003493   0.003340   0.003194   \n",
       "          0.2                   0.003462   0.003311   0.003166   0.003028   \n",
       "...                                  ...        ...        ...        ...   \n",
       "          4.8500000000000005    0.000202   0.000193   0.000184   0.000176   \n",
       "          4.9                   0.000198   0.000190   0.000181   0.000173   \n",
       "          4.95                  0.000195   0.000186   0.000178   0.000170   \n",
       "          5.0                   0.000192   0.000183   0.000175   0.000168   \n",
       "k_grid                         43.843880  44.569895  45.307931  46.058190   \n",
       "\n",
       "                                     796        797        798        799  \\\n",
       "data_type redshift                                                          \n",
       "Plin      0.0                   0.003585   0.003428   0.003277   0.003134   \n",
       "          0.05                  0.003399   0.003250   0.003108   0.002972   \n",
       "          0.1                   0.003222   0.003081   0.002946   0.002817   \n",
       "          0.15000000000000002   0.003054   0.002920   0.002792   0.002670   \n",
       "          0.2                   0.002895   0.002768   0.002647   0.002531   \n",
       "...                                  ...        ...        ...        ...   \n",
       "          4.8500000000000005    0.000169   0.000161   0.000154   0.000147   \n",
       "          4.9                   0.000166   0.000158   0.000152   0.000145   \n",
       "          4.95                  0.000163   0.000156   0.000149   0.000142   \n",
       "          5.0                   0.000160   0.000153   0.000147   0.000140   \n",
       "k_grid                         46.820871  47.596182  48.384332  49.185532   \n",
       "\n",
       "                                     800  \n",
       "data_type redshift                        \n",
       "Plin      0.0                   0.002997  \n",
       "          0.05                  0.002841  \n",
       "          0.1                   0.002693  \n",
       "          0.15000000000000002   0.002553  \n",
       "          0.2                   0.002420  \n",
       "...                                  ...  \n",
       "          4.8500000000000005    0.000141  \n",
       "          4.9                   0.000139  \n",
       "          4.95                  0.000136  \n",
       "          5.0                   0.000134  \n",
       "k_grid                         50.000000  \n",
       "\n",
       "[102 rows x 800 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_ref = FrameConstructor.create_reference_dataframe()\n",
    "dataframe_ref"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the reference dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe_ref.to_csv(\"../data/rwth_output/class_Asw0wa_DP/Plin_ref.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Normalize the data and divide it into training, validation and test sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load looti module datahandle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from looti import datahandle as dhl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify path and names of the pandas dataframes we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/rwth_output/class_Asw0wa_DP/' \n",
    "datafile_ext = 'Plin'\n",
    "datafile_ref = 'Plin_ref'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataHandle object and read the csv files containing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data = dhl.DataHandle(datafile_ext,                   # file name of csv file containing input data\n",
    "                                data_folder,                    # path to folder containing both external and reference dataframes\n",
    "                                datafile_ref,                   # file name of csv file containing reference data\n",
    "                                num_parameters=7,               # number of parameters to be interpolated\n",
    "                                data_type='Plin',               # type of observable to be emulated\n",
    "                                features_name='k_grid',         # name of the grid in the data frames\n",
    "                                features_to_Log=True,           # \n",
    "                                normalize_by_reference=True) \n",
    "emulation_data.read_csv_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data (by dividing through the reference and then normalizing by mean and standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data.calculate_ratio_by_redshifts(emulation_data.z_vals, normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the desired number of training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "\n",
    "n_train = 200                                               # Number of training vectors without taking acount the extrema \n",
    "n_test = 20                                                 # Number of test vectors without taking acount the extrema\n",
    "test_indices=[random.sample(range(1, 1000), n_test)]        # List of list -of test indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training, validation and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of wanted training vectors', 200)\n",
      "('number of wanted test vectors', 20)\n",
      "('redshift used for training', array([0.]))\n",
      "('redshfit used for testing', array([0.]))\n"
     ]
    }
   ],
   "source": [
    "emulation_data.calculate_data_split(n_train=n_train,                            # Number of training vectors per redshift\n",
    "                                    n_test=n_test,                              # Number of test vectors per redsift\n",
    "                                    verbosity=3,\n",
    "                                    manual_split=True,\n",
    "                                    test_indices=None,\n",
    "                                    train_redshift_indices=[0],      # Indices of the redshifts used for the train vect.\n",
    "                                    test_redshift_indices=[0])       # Indices of the redshifts used for the test vect."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) PCA and GP interpolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the looti module dictlearn which is used for PCA transformation and the training of intepolators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from looti import dictlearn as dcl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the number of PCA components and specify the number of varying parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "npca = 9\n",
    "nparam = 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is transformed into PCA space and a Gaussian Process (GP) is trained on the PCA components. \n",
    "<br>\n",
    "<br>\n",
    "Required input:\n",
    "- emulation_data\n",
    "\n",
    "\n",
    "Optional input:\n",
    "- Operator: type of interpolation (default = 'PCA')\n",
    "    - LIN: interpolation according to a simple spline\n",
    "    - PCA: interpolation over PCA components\n",
    "    - DL: interpolation over dictionary components of sparse DL representation\n",
    "    - GP: directly train a Gaussian Process on the data without further pre-processing\n",
    "- interp_type: type of interpolator\n",
    "    - GP: Gaussian Process\n",
    "    - int1d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of PCA matrix: (202, 9)',)\n",
      "('Number of PCA components: 9',)\n"
     ]
    }
   ],
   "source": [
    "ratios_predicted, emulation_data, intobj = dcl.Predict_ratio(emulation_data,Operator=\"PCA\",\n",
    "                                                             train_noise=1e-10,                     # Noise for the GP's kernel\n",
    "                                                             gp_n_rsts=40,                          # max times to restart the optimiser\n",
    "                                                             ncomp=npca,                            # Number of components\n",
    "                                                             gp_const=1,                            # Constant for the RBF kernel\n",
    "                                                             gp_length=np.ones(nparam) ,            # Length for GP \n",
    "                                                             interp_type='GP',                      # Kind of interpolator\n",
    "                                                             test_indices=test_indices,             # Indices of test vectors\n",
    "                                                             interp_dim=1,\n",
    "                                                             return_interpolator=True,\n",
    "                                                             pca_norm=True,\n",
    "                                                             train_redshift_indices=[0],\n",
    "                                                             test_redshift_indices=[0]\n",
    "                                                             # min_k =1e-2,ma_k=10e1\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32219, 800)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emulation_data.matrix_ratios_dict.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Save the trained interpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../interpolators/class_Asw0wa_DP/'\n",
    "intobj_name = 'Plin_z0.sav'\n",
    "data_name = 'Plin__z0_data.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(intobj, open(save_path+intobj_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(emulation_data, open(save_path+data_name, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now repeat the same procedure for the other observables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Non-linear power spectrum $P_{nonlin}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameConstructor = rf.FrameConstructor(path_config_file='../readfile_configs/input4cast_lhs_Pnonlin.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_Pnonlin_ext = FrameConstructor.create_k_dataframe()\n",
    "dataframe_Pnonlin_ext.to_csv(\"../data/rwth_output/class_Asw0wa_DP/Pnonlin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_Pnonlin_ref = FrameConstructor.create_k_reference_dataframe()\n",
    "dataframe_Pnonlin_ref.to_csv(\"../data/rwth_output/class_Asw0wa_DP/Pnonlin_ref.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/rwth_output/class_Asw0wa_DP/' \n",
    "datafile_ext = 'Pnonlin'\n",
    "datafile_ref = 'Pnonlin_ref'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data = dhl.DataHandle(datafile_ext,                   # file name of csv file containing input data\n",
    "                                data_folder,                    # path to folder containing both external and reference dataframes\n",
    "                                datafile_ref,                   # file name of csv file containing reference data\n",
    "                                num_parameters=7,               # number of parameters to be interpolated\n",
    "                                data_type='Pnonlin',            # type of observable to be emulated\n",
    "                                features_name='k_grid',         # name of the grid in the data frames\n",
    "                                features_to_Log=True,           # \n",
    "                                normalize_by_reference=True) \n",
    "\n",
    "emulation_data.read_csv_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data.calculate_ratio_by_redshifts(emulation_data.z_vals, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100                                                  # Number of training vectors without taking acount the extrema \n",
    "n_test = 2                                                     # Number of test vectors without taking acount the extrema\n",
    "test_indices=[random.sample(range(1, 1000), n_test)]           # List of list -of test indices, one list per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of wanted training vectors', 100)\n",
      "('number of wanted test vectors', 2)\n",
      "('redshift used for training', array([0.  , 0.05, 0.1 , 0.15, 0.2 ]))\n",
      "('redshfit used for testing', array([0.  , 0.05, 0.1 , 0.15, 0.2 ]))\n"
     ]
    }
   ],
   "source": [
    "emulation_data.calculate_data_split(n_train=n_train,                            # Number of training vectors per redshift\n",
    "                                    n_test=n_test,                              # Number of test vectors per redsift\n",
    "                                    verbosity=3,\n",
    "                                    manual_split=True,\n",
    "                                    test_indices=None,\n",
    "                                    train_redshift_indices=list(range(5)),      # Indices of the redshifts used for the train vect.\n",
    "                                    test_redshift_indices=list(range(5)))       # Indices of the redshifts used for the test vect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "npca = 7\n",
    "nparam = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of PCA matrix: (510, 7)',)\n",
      "('Number of PCA components: 7',)\n"
     ]
    }
   ],
   "source": [
    "ratios_predicted, emulation_data, intobj = dcl.Predict_ratio(emulation_data,Operator=\"PCA\",\n",
    "                                                             train_noise=1e-10,                     # Noise for the GP's kernel\n",
    "                                                             gp_n_rsts=40,                          # max times to restart the optimiser\n",
    "                                                             ncomp=npca,                            # Number of components\n",
    "                                                             gp_const=1,                            # Constant for the RBF kernel\n",
    "                                                             gp_length=np.ones(nparam) ,            # Length for GP \n",
    "                                                             interp_type='GP',                      # Kind of interpolator\n",
    "                                                             test_indices=test_indices,             # Indices of test vectors\n",
    "                                                             interp_dim=1,\n",
    "                                                             return_interpolator=True,\n",
    "                                                             pca_norm=True,\n",
    "                                                             train_redshift_indices=list(range(5)),\n",
    "                                                             test_redshift_indices=list(range(5))\n",
    "                                                             # min_k =1e-2,ma_k=10e1\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = '../interpolators/class_Asw0wa_DP/'\n",
    "save_name = 'Pnonlin.sav'\n",
    "data_name = 'Pnonlin_data.sav'\n",
    "pickle.dump(intobj, open(save_path+save_name, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Growth factor $D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from looti import datahandle as dhl\n",
    "from looti import dictlearn as dcl\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameConstructor = rf.FrameConstructor(path_config_file='../readfile_configs/input4cast_lhs_D_Growth.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_D_Growth_ext = FrameConstructor.create_k_dataframe()\n",
    "dataframe_D_Growth_ext.to_csv(\"../data/rwth_output/class_Asw0wa_DP/D_Growth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_D_Growth_ref = FrameConstructor.create_k_reference_dataframe()\n",
    "dataframe_D_Growth_ref.to_csv(\"../data/rwth_output/class_Asw0wa_DP/D_Growth_ref.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/rwth_output/class_Asw0wa_DP/'\n",
    "datafile_ext = 'D_Growth'\n",
    "datafile_ref = 'D_Growth_ref'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data = dhl.DataHandle(datafile_ext,                   # file name of csv file containing input data\n",
    "                                data_folder,                    # path to folder containing both external and reference dataframes\n",
    "                                datafile_ref,                   # file name of csv file containing reference data\n",
    "                                num_parameters=7,               # number of parameters to be interpolated\n",
    "                                data_type='D_Growth',           # type of observable to be emulated\n",
    "                                features_name='k_grid',         # name of the grid in the data frames\n",
    "                                features_to_Log=True,           # \n",
    "                                normalize_by_reference=True) \n",
    "emulation_data.read_csv_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data.calculate_ratio_by_redshifts(emulation_data.z_vals, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100                                                  # Number of training vectors without taking acount the extrema \n",
    "n_test = 2                                                     # Number of test vectors without taking acount the extrema\n",
    "test_indices=[random.sample(range(1, 1000), n_test)]           # List of list -of test indices, one list per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of wanted training vectors', 100)\n",
      "('number of wanted test vectors', 2)\n",
      "('redshift used for training', array([0.  , 0.05, 0.1 , 0.15, 0.2 ]))\n",
      "('redshfit used for testing', array([0.  , 0.05, 0.1 , 0.15, 0.2 ]))\n"
     ]
    }
   ],
   "source": [
    "emulation_data.calculate_data_split(n_train=n_train,                            # Number of training vectors per redshift\n",
    "                                    n_test=n_test,                              # Number of test vectors per redsift\n",
    "                                    verbosity=3,\n",
    "                                    manual_split=True,\n",
    "                                    test_indices=None,\n",
    "                                    train_redshift_indices=list(range(5)),      # Indices of the redshifts used for the train vect.\n",
    "                                    test_redshift_indices=list(range(5)))       # Indices of the redshifts used for the test vect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "npca = 7\n",
    "nparam = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of PCA matrix: (510, 7)',)\n",
      "('Number of PCA components: 7',)\n"
     ]
    }
   ],
   "source": [
    "ratios_predicted, emulation_data, intobj = dcl.Predict_ratio(emulation_data,Operator=\"PCA\",\n",
    "                                                             train_noise=1e-10,                     # Noise for the GP's kernel\n",
    "                                                             gp_n_rsts=40,                          # max times to restart the optimiser\n",
    "                                                             ncomp=npca,                            # Number of components\n",
    "                                                             gp_const=1,                            # Constant for the RBF kernel\n",
    "                                                             gp_length=np.ones(nparam) ,            # Length for GP \n",
    "                                                             interp_type='GP',                      # Kind of interpolator\n",
    "                                                             test_indices=test_indices,             # Indices of test vectors\n",
    "                                                             interp_dim=1,\n",
    "                                                             return_interpolator=True,\n",
    "                                                             pca_norm=True,\n",
    "                                                             train_redshift_indices=list(range(5)),\n",
    "                                                             test_redshift_indices=list(range(5))\n",
    "                                                             # min_k =1e-2,ma_k=10e1\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = '../interpolators/class_Asw0wa_DP/'\n",
    "save_name = 'D_Growth.sav'\n",
    "data_name = 'D_Growth_data.sav'\n",
    "pickle.dump(intobj, open(save_path+save_name, 'wb'))\n",
    "pickle.dump(emulation_data, open(save_path+data_name, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Growth rate $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from looti import datahandle as dhl\n",
    "from looti import dictlearn as dcl\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameConstructor = rf.FrameConstructor(path_config_file='../readfile_configs/input4cast_lhs_f_GrowthRate.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_f_GrowthRate_ext = FrameConstructor.create_k_dataframe()\n",
    "dataframe_f_GrowthRate_ext.to_csv(\"../data/rwth_output/class_Asw0wa_DP/f_GrowthRate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_f_GrowthRate_ref = FrameConstructor.create_k_reference_dataframe()\n",
    "dataframe_f_GrowthRate_ref.to_csv(\"../data/rwth_output/class_Asw0wa_DP/f_GrowthRate_ref.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/rwth_output/class_Asw0wa_DP/'\n",
    "datafile_ext = 'f_GrowthRate'\n",
    "datafile_ref = 'f_GrowthRate_ref'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data = dhl.DataHandle(datafile_ext,                   # file name of csv file containing input data\n",
    "                                data_folder,                    # path to folder containing both external and reference dataframes\n",
    "                                datafile_ref,                   # file name of csv file containing reference data\n",
    "                                num_parameters=7,               # number of parameters to be interpolated\n",
    "                                data_type='f_GrowthRate',       # type of observable to be emulated\n",
    "                                features_name='k_grid',         # name of the grid in the data frames\n",
    "                                features_to_Log=True,           # \n",
    "                                normalize_by_reference=True) \n",
    "emulation_data.read_csv_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data.calculate_ratio_by_redshifts(emulation_data.z_vals, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100                                                  # Number of training vectors without taking acount the extrema \n",
    "n_test = 2                                                     # Number of test vectors without taking acount the extrema\n",
    "test_indices=[random.sample(range(1, 1000), n_test)]           # List of list -of test indices, one list per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of wanted training vectors', 100)\n",
      "('number of wanted test vectors', 2)\n",
      "('redshift used for training', array([0.  , 0.05, 0.1 , 0.15, 0.2 ]))\n",
      "('redshfit used for testing', array([0.  , 0.05, 0.1 , 0.15, 0.2 ]))\n"
     ]
    }
   ],
   "source": [
    "emulation_data.calculate_data_split(n_train=n_train,                            # Number of training vectors per redshift\n",
    "                                    n_test=n_test,                              # Number of test vectors per redsift\n",
    "                                    verbosity=3,\n",
    "                                    manual_split=True,\n",
    "                                    test_indices=None,\n",
    "                                    train_redshift_indices=list(range(5)),      # Indices of the redshifts used for the train vect.\n",
    "                                    test_redshift_indices=list(range(5)))       # Indices of the redshifts used for the test vect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "npca = 7\n",
    "nparam = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of PCA matrix: (510, 7)',)\n",
      "('Number of PCA components: 7',)\n"
     ]
    }
   ],
   "source": [
    "ratios_predicted, emulation_data, intobj = dcl.Predict_ratio(emulation_data,Operator=\"PCA\",\n",
    "                                                             train_noise=1e-10,                     # Noise for the GP's kernel\n",
    "                                                             gp_n_rsts=40,                          # max times to restart the optimiser\n",
    "                                                             ncomp=npca,                            # Number of components\n",
    "                                                             gp_const=1,                            # Constant for the RBF kernel\n",
    "                                                             gp_length=np.ones(nparam) ,            # Length for GP \n",
    "                                                             interp_type='GP',                      # Kind of interpolator\n",
    "                                                             test_indices=test_indices,             # Indices of test vectors\n",
    "                                                             interp_dim=1,\n",
    "                                                             return_interpolator=True,\n",
    "                                                             pca_norm=True,\n",
    "                                                             train_redshift_indices=list(range(5)),\n",
    "                                                             test_redshift_indices=list(range(5))\n",
    "                                                             # min_k =1e-2,ma_k=10e1\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = '../interpolators/class_Asw0wa_DP/'\n",
    "save_name = 'f_GrowthRate.sav'\n",
    "data_name = 'f_GrowthRate_data.sav'\n",
    "pickle.dump(intobj, open(save_path+save_name, 'wb'))\n",
    "pickle.dump(emulation_data, open(save_path+data_name, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: $\\Sigma_{WL}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from looti import datahandle as dhl\n",
    "from looti import dictlearn as dcl\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "FrameConstructor = rf.FrameConstructor(path_config_file='../readfile_configs/input4cast_lhs_sigmaWL.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_sigmaWL_ext = FrameConstructor.create_k_dataframe()\n",
    "dataframe_sigmaWL_ext.to_csv(\"../data/rwth_output/class_Asw0wa_DP/sigmaWL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_sigmaWL_ref = FrameConstructor.create_k_reference_dataframe()\n",
    "dataframe_sigmaWL_ref.to_csv(\"../data/rwth_output/class_Asw0wa_DP/sigmaWL_ref.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/rwth_output/class_Asw0wa_DP/'\n",
    "datafile_ext = 'sigmaWL'\n",
    "datafile_ref = 'sigmaWL_ref'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data = dhl.DataHandle(datafile_ext,                   # file name of csv file containing input data\n",
    "                                data_folder,                    # path to folder containing both external and reference dataframes\n",
    "                                datafile_ref,                   # file name of csv file containing reference data\n",
    "                                num_parameters=7,               # number of parameters to be interpolated\n",
    "                                data_type='sigmaWL',           # type of observable to be emulated\n",
    "                                features_name='k_grid',         # name of the grid in the data frames\n",
    "                                features_to_Log=True,           # \n",
    "                                normalize_by_reference=True) \n",
    "emulation_data.read_csv_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulation_data.calculate_ratio_by_redshifts(emulation_data.z_vals, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100                                                  # Number of training vectors without taking acount the extrema \n",
    "n_test = 2                                                     # Number of test vectors without taking acount the extrema\n",
    "test_indices=[random.sample(range(1, 1000), n_test)]           # List of list -of test indices, one list per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of wanted training vectors', 100)\n",
      "('number of wanted test vectors', 2)\n",
      "('redshift used for training', array([0.  , 0.05, 0.1 , 0.15, 0.2 ]))\n",
      "('redshfit used for testing', array([0.  , 0.05, 0.1 , 0.15, 0.2 ]))\n"
     ]
    }
   ],
   "source": [
    "emulation_data.calculate_data_split(n_train=n_train,                            # Number of training vectors per redshift\n",
    "                                    n_test=n_test,                              # Number of test vectors per redsift\n",
    "                                    verbosity=3,\n",
    "                                    manual_split=True,\n",
    "                                    test_indices=None,\n",
    "                                    train_redshift_indices=list(range(5)),      # Indices of the redshifts used for the train vect.\n",
    "                                    test_redshift_indices=list(range(5)))       # Indices of the redshifts used for the test vect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "npca = 7\n",
    "nparam = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emulation_data.matrix_ratios_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ratios_predicted, emulation_data, intobj \u001b[39m=\u001b[39m dcl\u001b[39m.\u001b[39mPredict_ratio(emulation_data,Operator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPCA\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                                              train_noise\u001b[39m=\u001b[39m\u001b[39m1e-10\u001b[39m,                     \u001b[39m# Noise for the GP's kernel\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                                                              gp_n_rsts\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m,                          \u001b[39m# max times to restart the optimiser\u001b[39;00m\n\u001b[1;32m      4\u001b[0m                                                              ncomp\u001b[39m=\u001b[39mnpca,                            \u001b[39m# Number of components\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                                                              gp_const\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,                            \u001b[39m# Constant for the RBF kernel\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                                                              gp_length\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mones(nparam) ,            \u001b[39m# Length for GP \u001b[39;00m\n\u001b[1;32m      7\u001b[0m                                                              interp_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGP\u001b[39m\u001b[39m'\u001b[39m,                      \u001b[39m# Kind of interpolator\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                                                              test_indices\u001b[39m=\u001b[39mtest_indices,             \u001b[39m# Indices of test vectors\u001b[39;00m\n\u001b[1;32m      9\u001b[0m                                                              interp_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                                              return_interpolator\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m                                                              pca_norm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                                                              train_redshift_indices\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)),\n\u001b[1;32m     13\u001b[0m                                                              test_redshift_indices\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m))\n\u001b[1;32m     14\u001b[0m                                                              \u001b[39m# min_k =1e-2,ma_k=10e1\u001b[39;00m\n\u001b[1;32m     15\u001b[0m                                                              )\n",
      "File \u001b[0;32m~/looti/looti/dictlearn.py:536\u001b[0m, in \u001b[0;36mPredict_ratio\u001b[0;34m(emulation_data, Operator, ncomp, train_noise, gp_n_rsts, gp_const, gp_length, gp_bounds, interp_type, interp_dim, n_train, n_test, n_splits, split, test_indices, train_redshift_indices, test_redshift_indices, return_interpolator, thinning, min_k, max_k, mask, pca_norm)\u001b[0m\n\u001b[1;32m    534\u001b[0m     intobj \u001b[39m=\u001b[39m LearnData(PCAop)\n\u001b[1;32m    535\u001b[0m \u001b[39m###Perfoming the PCA reduction and interpolation\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m     intobj\u001b[39m.\u001b[39minterpolate(train_data\u001b[39m=\u001b[39memulation_data\u001b[39m.\u001b[39mmatrix_datalearn_dict[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    537\u001b[0m                        train_samples\u001b[39m=\u001b[39memulation_data\u001b[39m.\u001b[39mtrain_samples,train_noise \u001b[39m=\u001b[39m train_noise, pca_norm\u001b[39m=\u001b[39mpca_norm)\n\u001b[1;32m    538\u001b[0m     ratios_predicted \u001b[39m=\u001b[39m intobj\u001b[39m.\u001b[39mpredict(emulation_data\u001b[39m.\u001b[39mtest_samples, pca_norm)\n\u001b[1;32m    540\u001b[0m     \u001b[39mif\u001b[39;00m return_interpolator \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/looti/looti/dictlearn.py:123\u001b[0m, in \u001b[0;36mLearnData.interpolate\u001b[0;34m(self, train_data, train_samples, train_noise, pca_norm)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLINtraining()\n\u001b[1;32m    122\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPCA\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPCAtraining(pca_norm)\n\u001b[1;32m    124\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDL\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDLtraining()\n",
      "File \u001b[0;32m~/looti/looti/dictlearn.py:149\u001b[0m, in \u001b[0;36mLearnData.PCAtraining\u001b[0;34m(self, pca_norm)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpca\u001b[39m=\u001b[39mpca\u001b[39m## take n principal components\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m pca_norm \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     matPCA_raw\u001b[39m=\u001b[39mpca\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainspace_mat)\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainspace_mat)\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatPCA_mean \u001b[39m=\u001b[39m matPCA_raw\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatPCA_std \u001b[39m=\u001b[39m matPCA_raw\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/lootiemu/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:435\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X)\n\u001b[1;32m    436\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/lootiemu/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[1;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPCA does not support sparse input. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTruncatedSVD for a possible alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 485\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    486\u001b[0m     X, dtype\u001b[39m=\u001b[39m[np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32], ensure_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, copy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m    487\u001b[0m )\n\u001b[1;32m    489\u001b[0m \u001b[39m# Handle n_components==None\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lootiemu/lib/python3.11/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/miniconda3/envs/lootiemu/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniconda3/envs/lootiemu/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "ratios_predicted, emulation_data, intobj = dcl.Predict_ratio(emulation_data,Operator=\"PCA\",\n",
    "                                                             train_noise=1e-10,                     # Noise for the GP's kernel\n",
    "                                                             gp_n_rsts=40,                          # max times to restart the optimiser\n",
    "                                                             ncomp=npca,                            # Number of components\n",
    "                                                             gp_const=1,                            # Constant for the RBF kernel\n",
    "                                                             gp_length=np.ones(nparam) ,            # Length for GP \n",
    "                                                             interp_type='GP',                      # Kind of interpolator\n",
    "                                                             test_indices=test_indices,             # Indices of test vectors\n",
    "                                                             interp_dim=1,\n",
    "                                                             return_interpolator=True,\n",
    "                                                             pca_norm=True,\n",
    "                                                             train_redshift_indices=list(range(5)),\n",
    "                                                             test_redshift_indices=list(range(5))\n",
    "                                                             # min_k =1e-2,ma_k=10e1\n",
    "                                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = '../interpolators/class_Asw0wa_DP/'\n",
    "save_name = 'sigmaWL.sav'\n",
    "data_name = 'sigmaWL_data.sav'\n",
    "pickle.dump(intobj, open(save_path+save_name, 'wb'))\n",
    "pickle.dump(emulation_data, open(save_path+data_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lootiemu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
